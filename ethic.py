# -*- coding: utf-8 -*-
"""ethic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13lHc4XlGWbhmUnvomoZl-MwIVfpyIMEY
"""


import fitz  # PyMuPDF
import pandas as pd

# Open the PDF file
pdf_path = '/content/Sunan an-Nasai Vol. 1 - 1-876.pdf'
pdf_document = fitz.open(pdf_path)

# Initialize lists to store page number and text content
page_numbers = []
page_texts = []

# Loop through each page to extract text
for page_num in range(len(pdf_document)):
    page = pdf_document[page_num]
    text = page.get_text()

    # Append the extracted text and page number to lists
    page_numbers.append(page_num + 1)
    page_texts.append(text)

# Close the PDF document
pdf_document.close()

# Create a DataFrame from extracted data
df = pd.DataFrame({
    'Page': page_numbers,
    'Content': page_texts
})

# Save DataFrame to a CSV file
csv_path = 'sunun ib nasi.csv'
df.to_csv(csv_path, index=False)

print(f"CSV file saved at: {csv_path}")


import pandas as pd

# Load the CSV
df = pd.read_csv('/content/sunun ib nasi.csv')

# Clean up special characters from content column
df['Content'] = df['Content'].apply(lambda x: x.replace('\n', ' ').replace('\r', '').replace('"', '') if isinstance(x, str) else x)

# Save cleaned data to a new CSV file
cleaned_csv_path = 'new.csv'
df.to_csv(cleaned_csv_path, index=False)

print(f"Cleaned CSV file saved at: {cleaned_csv_path}")

import pandas as pd

# Load the dataset
df = pd.read_csv('/content/new.csv')

# Drop columns with mixed types
df = df.loc[:, df.applymap(type).nunique() == 1]

import fitz  # PyMuPDF

# Function to extract text from a PDF file
def extract_text_from_pdf(pdf_path):
    text = ""
    with fitz.open(pdf_path) as pdf_document:
        for page in pdf_document:
            text += page.get_text()
    return text

# Path to your PDF
pdf_path = '/content/Sunan an-Nasai Vol. 1 - 1-876.pdf'
pdf_text = extract_text_from_pdf(pdf_path)

# Save the extracted text to a file for later processing (optional)
with open("sunan_an_nasai_text.txt", "w", encoding='utf-8') as f:
    f.write(pdf_text)



from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering

# Load the tokenizer and model
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForQuestionAnswering.from_pretrained("distilbert-base-uncased")

import torch
from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
import pandas as pd

# Load the tokenizer and model
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForQuestionAnswering.from_pretrained("distilbert-base-uncased")

# Tokenization function
def tokenize_data(examples):
    questions = examples['Questions'].tolist()
    contexts = examples['Content'].tolist()

    encodings = tokenizer(
        questions,
        contexts,
        truncation=True,
        padding='max_length',
        return_tensors='pt'
    )

    # Create dummy positions; these should be filled in with actual start and end indices
    start_positions = [0] * len(contexts)  # Replace with actual start indices if available
    end_positions = [0] * len(contexts)    # Replace with actual end indices if available

    encodings['start_positions'] = torch.tensor(start_positions)
    encodings['end_positions'] = torch.tensor(end_positions)

    return encodings

# Split your data
train_data, val_data = train_test_split(data, test_size=0.1)

# Tokenize the train and validation sets
train_encodings = tokenize_data(train_data)
val_encodings = tokenize_data(val_data)

# Define a Dataset class for PyTorch
class QA_Dataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.encodings.items()}
        return item

    def __len__(self):
        return len(self.encodings['input_ids'])

train_dataset = QA_Dataset(train_encodings)
val_dataset = QA_Dataset(val_encodings)

# Training arguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    report_to=[],  # Disable W&B logging
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)

# Train the model
trainer.train()

# Save the model and tokenizer
model.save_pretrained("./distilbert-finetuned")
tokenizer.save_pretrained("./distilbert-finetuned")

import pdfplumber
import re

# Function to extract text from PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
    return text

# Function to find Hadiths related to the chapter "Cleaning Oneself With Water"
def find_hadiths_related_to_chapter(chapter_title, pdf_text):
    # Split the text based on chapters or specific keywords
    chapter_pattern = re.compile(rf'{chapter_title}\s*\.+(.*?)(?=\n\d+\s*\.)', re.DOTALL)
    matches = chapter_pattern.findall(pdf_text)

    # Collect the relevant Hadiths
    relevant_hadiths = []

    if matches:
        # Flatten the matches since they might be in lists
        for match in matches:
            hadiths = match.strip().split('\n')
            relevant_hadiths.extend(hadiths)
            if len(relevant_hadiths) >= 4:  # Limit to three Hadiths
                break

    return relevant_hadiths[:4]  # Return only the first three Hadiths

# Extract text from your PDF file
pdf_path = "/content/Sunan an-Nasai Vol. 1 - 1-876.pdf"
pdf_text = extract_text_from_pdf(pdf_path)

# Define the chapter title to search
chapter_title = "Cleaning Oneself With Water"

# Find relevant Hadiths
found_hadiths = find_hadiths_related_to_chapter(chapter_title, pdf_text)

# Display relevant Hadiths
if found_hadiths:
    print(f"Relevant Hadiths about '{chapter_title}':")
    for hadith in found_hadiths:
        print(hadith.strip())
else:
    print(f"No relevant Hadiths found about '{chapter_title}'.")

import pdfplumber
import re

# Function to extract and clean text from PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                cleaned_text = re.sub(r'\s+', ' ', page_text)  # Clean extra spaces
                text += cleaned_text.strip() + "\n"
    return text

# Function to check if a line contains Arabic characters
def contains_arabic(line):
    return re.search(r'[\u0600-\u06FF]', line) is not None

# Function to find and format Hadiths
def find_hadiths_related_to_chapter(chapter_title, pdf_text):
    relevant_hadiths = []
    chapter_found = False
    hadith_text = ""
    narration = []

    # Split text into lines for processing
    lines = pdf_text.split('\n')

    for line in lines:
        print(f"Processing line: {line}")  # Debug print
        if chapter_title.lower() in line.lower():
            print("Chapter found!")  # Debug message
            chapter_found = True
            continue

        if re.match(r'^\d+\s+\.', line) and chapter_found:
            if hadith_text and narration:
                # Filter to include only English narration
                filtered_narration = [text for text in narration if not contains_arabic(text)]
                relevant_hadiths.append(f"{hadith_text}: {' '.join(filtered_narration)}")
            hadith_text = line.strip()  # New Hadith
            narration = []  # Reset for narration
            print(f"New Hadith detected: {hadith_text}")  # Debug message
        elif chapter_found:
            if line.strip() and not re.match(r'^\d+\s+\.', line):
                narration.append(line.strip())

    # Handle the last hadith
    if hadith_text and narration:
        filtered_narration = [text for text in narration if not contains_arabic(text)]
        relevant_hadiths.append(f"{hadith_text}: {' '.join(filtered_narration)}")

    return relevant_hadiths[:3]

# Example usage
pdf_path = "/content/Sunan an-Nasai Vol. 1 - 1-876.pdf"
pdf_text = extract_text_from_pdf(pdf_path)
chapter_title = "Cleaning Oneself With Water"
found_hadiths = find_hadiths_related_to_chapter(chapter_title, pdf_text)

# Display results
if found_hadiths:
    for hadith in found_hadiths:
        print(hadith)
else:
    print("No Hadiths found for the specified chapter.")

import pdfplumber
import re

# Function to extract text from PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
    return text

# Function to check if a line contains Arabic characters
def contains_arabic(line):
    return re.search(r'[\u0600-\u06FF]', line) is not None

# Function to find Hadiths related to a specific chapter
def find_hadiths_related_to_chapter(chapter_title, pdf_text):
    # Split the text based on chapters or specific keywords
    chapter_pattern = re.compile(rf'{chapter_title}\s*\.+(.*?)(?=\n\d+\s*\.)', re.DOTALL)
    matches = chapter_pattern.findall(pdf_text)

    # Collect the relevant Hadiths
    relevant_hadiths = []

    if matches:
        for match in matches:
            # Split into lines and filter for non-Arabic lines
            hadiths = match.strip().split('\n')
            filtered_hadiths = [line.strip() for line in hadiths if line.strip() and not contains_arabic(line)]
            relevant_hadiths.extend(filtered_hadiths)
            if len(relevant_hadiths) >= 4:  # Limit to three Hadiths
                break

    return relevant_hadiths[:4]  # Return only the first three Hadiths

# Extract text from your PDF file
pdf_path = "/content/Sunan an-Nasai Vol. 1 - 1-876.pdf"
pdf_text = extract_text_from_pdf(pdf_path)

# Define the chapter title to search
chapter_title = "Cleaning Oneself With Water"

# Find relevant Hadiths
found_hadiths = find_hadiths_related_to_chapter(chapter_title, pdf_text)

# Display relevant Hadiths
if found_hadiths:
    print(f"Relevant Hadiths about '{chapter_title}':")
    for hadith in found_hadiths:
        print(hadith.strip())
else:
    print(f"No relevant Hadiths found about '{chapter_title}'.")

import pdfplumber
import re
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
import torch

# Load your fine-tuned model and tokenizer
model_path = "/content/distilbert-finetuned"
tokenizer = DistilBertTokenizer.from_pretrained(model_path)
model = DistilBertForSequenceClassification.from_pretrained(model_path)

# Function to extract text from PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
    return text

# Function to find Hadiths related to a specific chapter
def find_hadiths_related_to_chapter(chapter_title, pdf_text):
    chapter_pattern = re.compile(rf'{chapter_title}\s*\.+(.*?)(?=\n\d+\s*\.)', re.DOTALL)
    matches = chapter_pattern.findall(pdf_text)

    # Collect the relevant Hadiths
    relevant_hadiths = []

    if matches:
        for match in matches:
            # Assuming English narration follows a specific pattern, adjust accordingly
            hadiths = match.strip().split('\n')
            for hadith in hadiths:
                # Check if there's an English narration pattern, e.g., "Narrated:"
                if "Narrated:" in hadith:
                    relevant_hadiths.append(hadith.strip())
                elif hadith:  # Include Arabic hadiths
                    relevant_hadiths.append(hadith.strip())

                if len(relevant_hadiths) >= 4:  # Limit to four Hadiths
                    break

    return relevant_hadiths[:4]  # Return only the first four Hadiths

# Extract text from your PDF file
pdf_path = "/content/Sunan an-Nasai Vol. 1 - 1-876.pdf"
pdf_text = extract_text_from_pdf(pdf_path)

# Define the chapter title to search
chapter_title = "Cleaning Oneself With Water"

# Find relevant Hadiths
found_hadiths = find_hadiths_related_to_chapter(chapter_title, pdf_text)

# Process and analyze relevant Hadiths with the model
if found_hadiths:
    print(f"Relevant Hadiths about '{chapter_title}':")
    for hadith in found_hadiths:
        print(hadith)  # Print the Hadith

        # Tokenization
        inputs = tokenizer(hadith, return_tensors='pt', padding=True, truncation=True)

        # Inference
        with torch.no_grad():
            outputs = model(**inputs)

        # Process the outputs (for example, take logits)
        logits = outputs.logits
        predicted_class = torch.argmax(logits, dim=1).item()  # Example of getting predicted class

        # Print or store the results
        print(f"Predicted class: {predicted_class}")
else:
    print(f"No relevant Hadiths found about '{chapter_title}'.")

import pdfplumber
import re
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
import torch

# Load your fine-tuned model and tokenizer
model_path = "/content/distilbert-finetuned"
tokenizer = DistilBertTokenizer.from_pretrained(model_path)
model = DistilBertForSequenceClassification.from_pretrained(model_path)

# Function to extract text from PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
    return text

# Function to find Hadiths related to a specific chapter
def find_hadiths_related_to_chapter(chapter_title, pdf_text):
    # Adjust the regex to capture the narration of Hadith
    chapter_pattern = re.compile(
        rf'{re.escape(chapter_title)}\s*\.+(.*?)(?=\n\d+\s*\.)', re.DOTALL)

    matches = chapter_pattern.findall(pdf_text)

    # Collect the relevant Hadiths
    relevant_hadiths = []

    if matches:
        for match in matches:
            hadiths = match.strip().split('\n')
            relevant_hadiths.extend(hadiths)
            if len(relevant_hadiths) >= 4:  # Limit to four Hadiths
                break

    return relevant_hadiths[:4]  # Return only the first four Hadiths

# Extract text from your PDF file
pdf_path = "/content/Sunan an-Nasai Vol. 1 - 1-876.pdf"
pdf_text = extract_text_from_pdf(pdf_path)

# Define the chapter title to search
chapter_title = "Cleaning Oneself With Water"

# Find relevant Hadiths
found_hadiths = find_hadiths_related_to_chapter(chapter_title, pdf_text)

# Process and analyze relevant Hadiths with the model
if found_hadiths:
    print(f"Relevant Hadiths about '{chapter_title}':")
    for hadith in found_hadiths:
        print(hadith.strip())

        # Tokenization
        inputs = tokenizer(hadith, return_tensors='pt', padding=True, truncation=True)

        # Inference
        with torch.no_grad():
            outputs = model(**inputs)

        # Process the outputs (for example, take logits)
        logits = outputs.logits
        predicted_class = torch.argmax(logits, dim=1).item()  # Example of getting predicted class

        # Print or store the results
        print(f"Predicted class: {predicted_class}")
else:
    print(f"No relevant Hadiths found about '{chapter_title}'.")

import streamlit as st
import pdfplumber
import re
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
import torch

# Load your fine-tuned model and tokenizer
model_path = "distilbert-finetuned"  # Adjust the path according to your environment
tokenizer = DistilBertTokenizer.from_pretrained(model_path)
model = DistilBertForSequenceClassification.from_pretrained(model_path)

# Function to extract text from PDF
def extract_text_from_pdf(pdf_file):
    text = ""
    with pdfplumber.open(pdf_file) as pdf:
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
    return text

# Function to find Hadiths related to a specific chapter
def find_hadiths_related_to_chapter(chapter_title, pdf_text):
    chapter_pattern = re.compile(
        rf'{re.escape(chapter_title)}\s*\.+(.*?)(?=\n\d+\s*\.)', re.DOTALL)

    matches = chapter_pattern.findall(pdf_text)
    relevant_hadiths = []

    if matches:
        for match in matches:
            hadiths = match.strip().split('\n')
            relevant_hadiths.extend(hadiths)
            if len(relevant_hadiths) >= 4:  # Limit to four Hadiths
                break

    return relevant_hadiths[:4]  # Return only the first four Hadiths

# Streamlit UI
st.title("Hadith Search Application")
st.subheader("Extract and Analyze Hadiths from a PDF")

# File uploader for PDF
pdf_file = st.file_uploader("Upload your PDF file", type=["pdf"])

# Input for chapter title
chapter_title = st.text_input("Enter Chapter Title")

if st.button("Find Hadiths"):
    if pdf_file and chapter_title:
        pdf_text = extract_text_from_pdf(pdf_file)
        found_hadiths = find_hadiths_related_to_chapter(chapter_title, pdf_text)

        if found_hadiths:
            st.write(f"Relevant Hadiths about '{chapter_title}':")
            for hadith in found_hadiths:
                st.write(hadith.strip())

                # Tokenization
                inputs = tokenizer(hadith, return_tensors='pt', padding=True, truncation=True)

                # Inference
                with torch.no_grad():
                    outputs = model(**inputs)

                # Process the outputs (for example, take logits)
                logits = outputs.logits
                predicted_class = torch.argmax(logits, dim=1).item()  # Get predicted class

                # Print or store the results
                st.write(f"Predicted class: {predicted_class}")
        else:
            st.write(f"No relevant Hadiths found about '{chapter_title}'.")
    else:
        st.warning("Please upload a PDF file and enter a chapter title.")



